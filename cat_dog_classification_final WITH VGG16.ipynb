{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "cat_dog_classification_final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepkumar101010/DLCVNLP/blob/main/cat_dog_classification_final%20WITH%20VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pointed-lambda"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "id": "pointed-lambda",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broad-athens"
      },
      "source": [
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "broad-athens",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnBMPHmyndiq",
        "outputId": "a529da73-87c5-4914-e152-603fddef7002"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "wnBMPHmyndiq",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collective-albert"
      },
      "source": [
        "#Input Size of image \n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "#training and valid path \n",
        "train_path = r'/content/drive/MyDrive/Dog_cat with Transfer learning/train'\n",
        "valid_path = r'/content/drive/MyDrive/Dog_cat with Transfer learning/test' "
      ],
      "id": "collective-albert",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "portable-bridge",
        "outputId": "12e8012b-3c3c-4193-b87a-911811acc59a"
      },
      "source": [
        "#pulling out trained weights from VGG16\n",
        "#include top = False as we will not use FC and O/p of VGG16 we will use till last max pooling \n",
        "#weights = imagenet name to get trained weights\n",
        "#input_shape = Shape input for model\n",
        "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
      ],
      "id": "portable-bridge",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advised-vault"
      },
      "source": [
        "# don't train existing weights\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable = False\n"
      ],
      "id": "advised-vault",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sticky-consciousness"
      },
      "source": [
        "#to get the number of output classes\n",
        "folders = glob('/content/drive/MyDrive/Dog_cat with Transfer learning\\*')"
      ],
      "id": "sticky-consciousness",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlbPnXY6oNOG",
        "outputId": "603bf0cc-807b-418a-a41a-4fb48b3fc654"
      },
      "source": [
        "folders"
      ],
      "id": "DlbPnXY6oNOG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affecting-capitol",
        "outputId": "8c02ef10-cb76-45a4-ef6d-c8f438038602"
      },
      "source": [
        "glob(r'D:\\anacondaenv\\training_set\\*')"
      ],
      "id": "affecting-capitol",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neutral-cocktail",
        "outputId": "8b96712b-fbda-4cfe-f851-a8272c9aa979"
      },
      "source": [
        "x = Flatten()(vgg.output)\n",
        "prediction = Dense(2, activation='softmax')(x)\n",
        "model = Model(inputs=vgg.input, outputs=prediction)\n",
        "model.summary()"
      ],
      "id": "neutral-cocktail",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 50178     \n",
            "=================================================================\n",
            "Total params: 14,764,866\n",
            "Trainable params: 50,178\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "permanent-margin"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "permanent-margin",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "humanitarian-butter"
      },
      "source": [
        "# Use the Image Data Generator to import the images from the dataset\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "id": "humanitarian-butter",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "based-decline",
        "outputId": "32504f21-76b3-442a-f14d-09b490d9613d"
      },
      "source": [
        "training_set = train_datagen.flow_from_directory(r'/content/drive/MyDrive/Dog_cat with Transfer learning/train',\n",
        "                                                 target_size=(224,224),\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode=\"categorical\")"
      ],
      "id": "based-decline",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18997 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "occupied-klein",
        "outputId": "ac82a5d7-bf02-4ef9-f53d-0bfbae3fe459"
      },
      "source": [
        "#Scaling test data before getting prediction from trained model\n",
        "test_set = test_datagen.flow_from_directory(r'/content/drive/MyDrive/Dog_cat with Transfer learning/test',\n",
        "                                            target_size=(224,224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode=\"categorical\")"
      ],
      "id": "occupied-klein",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6003 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bored-disclosure",
        "outputId": "f6a28290-525a-491a-9757-27cb9ab3ab7f"
      },
      "source": [
        "model.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=20,\n",
        "  steps_per_epoch=30,\n",
        "  validation_steps=50,\n",
        "   )\n"
      ],
      "id": "bored-disclosure",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 573s 19s/step - loss: 0.8664 - accuracy: 0.6521 - val_loss: 0.4533 - val_accuracy: 0.8119\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 486s 16s/step - loss: 0.4206 - accuracy: 0.8128 - val_loss: 0.2415 - val_accuracy: 0.9013\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 408s 14s/step - loss: 0.2462 - accuracy: 0.8907 - val_loss: 0.2418 - val_accuracy: 0.8950\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 367s 12s/step - loss: 0.2251 - accuracy: 0.9095 - val_loss: 0.2107 - val_accuracy: 0.9094\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 295s 10s/step - loss: 0.2222 - accuracy: 0.9086 - val_loss: 0.2471 - val_accuracy: 0.9000\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 269s 9s/step - loss: 0.3259 - accuracy: 0.8550 - val_loss: 0.4011 - val_accuracy: 0.8344\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 252s 9s/step - loss: 0.2399 - accuracy: 0.9028 - val_loss: 0.2295 - val_accuracy: 0.9038\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 218s 7s/step - loss: 0.2279 - accuracy: 0.9052 - val_loss: 0.3134 - val_accuracy: 0.8750\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 201s 7s/step - loss: 0.2466 - accuracy: 0.9015 - val_loss: 0.1954 - val_accuracy: 0.9212\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 184s 6s/step - loss: 0.1925 - accuracy: 0.9216 - val_loss: 0.2036 - val_accuracy: 0.9156\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 162s 5s/step - loss: 0.1900 - accuracy: 0.9159 - val_loss: 0.2023 - val_accuracy: 0.9225\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 162s 5s/step - loss: 0.1816 - accuracy: 0.9106 - val_loss: 0.3111 - val_accuracy: 0.8719\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 147s 5s/step - loss: 0.2256 - accuracy: 0.9003 - val_loss: 0.1658 - val_accuracy: 0.9300\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 135s 5s/step - loss: 0.2557 - accuracy: 0.8944 - val_loss: 0.2099 - val_accuracy: 0.9219\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 130s 4s/step - loss: 0.1704 - accuracy: 0.9310 - val_loss: 0.1725 - val_accuracy: 0.9331\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 120s 4s/step - loss: 0.2365 - accuracy: 0.9093 - val_loss: 0.2182 - val_accuracy: 0.9131\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 119s 4s/step - loss: 0.3108 - accuracy: 0.8718 - val_loss: 0.4145 - val_accuracy: 0.8656\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 115s 4s/step - loss: 0.3296 - accuracy: 0.8667 - val_loss: 0.2512 - val_accuracy: 0.9044\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 103s 3s/step - loss: 0.2777 - accuracy: 0.9078 - val_loss: 0.2283 - val_accuracy: 0.9156\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 103s 3s/step - loss: 0.2149 - accuracy: 0.9175 - val_loss: 0.2260 - val_accuracy: 0.9262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdefcf93d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJw6NqB_EfXl"
      },
      "source": [
        "#plotting training and validation loss \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(r.history['loss'], label = 'train_loss')\r\n",
        "plt.plot(r.history['val_loss'], label = 'val_loss')\r\n",
        "ply.legend()\r\n",
        "plt.show()\r\n",
        "plt.savefig('LossVal_loss')\r\n"
      ],
      "id": "kJw6NqB_EfXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVHV3AOAEf0c"
      },
      "source": [
        "#plotting training and validation accuracy\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(r.history['accuracy'], label = 'train_accuracy')\r\n",
        "plt.plot(r.history['val_accuracy'], label = 'val_accuracy')\r\n",
        "ply.legend()\r\n",
        "plt.show()\r\n",
        "plt.savefig('train_valid_accuracy')"
      ],
      "id": "CVHV3AOAEf0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k62iy-GkEf5L"
      },
      "source": [
        "\r\n",
        "# save it as a h5 file\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "\r\n",
        "cnn.save('model_rcat_transfer.h5')"
      ],
      "id": "k62iy-GkEf5L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1wWual1Ef9R",
        "outputId": "923c5fd3-e5fa-4512-c8ba-cb44f428f028"
      },
      "source": [
        "y_pred = model.predict(test_set)\r\n",
        "y_pred"
      ],
      "id": "k1wWual1Ef9R",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.5694101e-01, 4.3058988e-02],\n",
              "       [9.9887830e-01, 1.1216428e-03],\n",
              "       [4.4236522e-02, 9.5576346e-01],\n",
              "       ...,\n",
              "       [4.4169663e-03, 9.9558300e-01],\n",
              "       [2.6241744e-01, 7.3758262e-01],\n",
              "       [3.1996701e-15, 1.0000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38UQYLTLEgBT"
      },
      "source": [
        "# Part 4 - Making a single prediction\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "test_image = image.load_img('/content/drive/MyDrive/Dog_cat with Transfer learning/test/Dog/dog.1000.jpg', target_size = (224,224))\r\n",
        "test_image = image.img_to_array(test_image)\r\n",
        "test_image=test_image/255\r\n",
        "test_image = np.expand_dims(test_image, axis = 0)\r\n",
        "result = model.predict(test_image)"
      ],
      "id": "38UQYLTLEgBT",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHxHJ5LrFn5Q"
      },
      "source": [
        "#as we have sue softmax activation function so it will give probablity of every category so there wiil bee two \r\n",
        "#output like[0.9953,0.0047]\r\n",
        "a = np.argmax(result)"
      ],
      "id": "LHxHJ5LrFn5Q",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPWxOcpMEgDM",
        "outputId": "4f18dcc0-e55a-4147-89bc-55883f5ccf4e"
      },
      "source": [
        "#so a will contain values on two indices a[0],a[1]\r\n",
        "if a == [0]:\r\n",
        "    print(\"The image classified is Cat\")\r\n",
        "else:\r\n",
        "    print(\"The image classified is dog\")"
      ],
      "id": "WPWxOcpMEgDM",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The image classified is dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwRGyOsREgFP"
      },
      "source": [
        ""
      ],
      "id": "XwRGyOsREgFP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mrgsD3PEgHJ"
      },
      "source": [
        ""
      ],
      "id": "3mrgsD3PEgHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "602SI6r0EgIv"
      },
      "source": [
        ""
      ],
      "id": "602SI6r0EgIv",
      "execution_count": null,
      "outputs": []
    }
  ]
}